Last login: Sun Nov  3 18:46:20 on ttys000

The default interactive shell is now zsh.
To update your account to use zsh, please run `chsh -s /bin/zsh`.
For more details, please visit https://support.apple.com/kb/HT208050.
(base) Christians-MacBook-Pro-2:~ christian$ python3
Python 3.7.3 (default, Mar 27 2019, 16:54:48) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import os
>>> print(os.getcwd())
/Users/christian
>>> f = open("privacy.txt")
>>> f.read()
'\nIs privacy privacy?\nKobbi Nissim and AlexandraWood\n\nThis position paper observes how different technical\nand normative conceptions of privacy have evolved\nin parallel and describes the practical challenges\nthat these divergent approaches pose. Notably,\npast technologies relied on intuitive, heuristic\nunderstandings of privacy that have since been shown\nnot to satisfy expectations for privacy protection.With\ncomputations ubiquitously integrated in almost every\naspect of our lives, it is increasingly important to\nensure that privacy technologies provide protection\nthat is in line with relevant social norms and\nnormative expectations. Similarly, it is also important\nto examine social norms and normative expectations\nwith respect to the evolving scientific study of privacy.\nTo this end, we argue for a rigorous analysis of the\nmapping from normative to technical concepts of\nprivacy and vice versa. We review the landscape of\nnormative and technical definitions of privacy and\ndiscuss specific examples of gaps between definitions\nthat are relevant in the context of privacy in statistical\ncomputation. We then identify opportunities for\novercoming their differences in the design of new\napproaches to protecting privacy in accordance with\nboth technical and normative standards.\nThis article is part of a discussion meeting issue\n‘The growing ubiquity of algorithms in society:\nimplications, impacts and innovations’.\n1. Introduction\nPrivacy concerns regarding the collection, storage and\nuse of personal information are a recurring topic of\npublic discourse. Data analysis is now deeply and\nirrevocably embedded in our systems: from social\nnetworks, to electronic commerce, medical care, national\nsecurity, research and more. Analyses are being fed\nwith increasingly detailed information—including data\nabout our traits, our relationships, our behaviours, our interests and our preferences. Such\nanalyses, individually or in combination with other analyses, can reveal sensitive attributes\nof individuals, including information about their health, finances, political leanings and social\nbehaviours. The growing ubiquity of data analyses therefore implicates social norms about\nprivacy. Misuse or disclosure of such data can adversely affect an individual’s relationships,\nreputation, employability, insurability or financial status, or even lead to civil liability, criminal\npenalties or bodily harm. It can also have negative consequences for essential rights and social\nvalues, such as freedom of expression, freedom of association, and respect for private and family\nlife, more broadly [1,2].\nIn light of the attendant risks to individuals and to social values, organizations that manage\npersonal data implement various measures to protect the privacy of those whose personal\ninformation is used. Yet the data privacy landscape is rapidly evolving, as advances in technology\nchange how personal information is collected, stored, shared, analysed and disseminated.\nOrganizations increasingly rely on technical safeguards to address growing data privacy risks.\nA common premise is that these technical safeguards protect individual privacy in accordance\nwith legal and social norms. Privacy technologies are viewed through this lens because privacy\nis inherently a normative concept, with foundations in philosophical, legal, sociological, political\nand economic traditions. Various privacy regulations and policies attempt to capture and codify\nthese norms and values as enforceable constraints on behaviour. Examples can be found in\nlaws protecting educational records,1 medical records,2 information collected by the US Census\nBureau,3 information maintained by US federal agencies4 and personal data about individuals in\nthe European Union.5\nExpectations and understandings of privacy are dramatically shifting, in response to a\nwide spectrum of privacy-invasive technologies underlying applications in everyday use by\ngovernment agencies and businesses alike. A range of technical measures has been deployed to\nmitigate informational privacy risks. Common approaches include suppression, data swapping,\nnoise addition, synthetic data, aggregation via statistical and machine learning tools, query\nlogging and auditing, and more. With this expanded reliance on technological approaches,\nprivacy is increasingly becoming a technical concept, in addition to being a normative concept.\nFurthermore, the understanding of privacy reflected in these technologies is evolving over\ntime—partly in response to the discovery of new vulnerabilities, a process that has significantly\naccelerated in the last two decades. Key examples illustrating the evolution of privacy practices\ncan be found in the policies of federal statistical agencies, which have sought to strengthen\nprotections in response to newly identified threats to privacy. For instance, the US Census Bureau\nhas adopted different approaches to confidentiality over time, beginning with suppression and\naggregation, then introducing data swapping methods, and, most recently, exploring uses of\nformal privacy models like differential privacy [3,4].\nHowever, significant gaps between technical and normative conceptions of privacy pose\nchallenges for the development and deployment of privacy technologies. The technical language\nis often mathematical, precise and somewhat rigid whereas the language describing social norms\nis flexible and less precise. There are substantive differences in scope, in what is considered\npersonal information, in what is considered a privacy threat, in the protection required, and\nin the data formats the normative and technical conceptions contemplate. With such divergent\nfoundations, it is difficult to reason about how these conceptions of privacy interact, or, more\ncritically, whether they are in agreement at all.\n1Family Educational Rights and Privacy Act of 1974, 20 U.S.C. § 1232g; 34 C.F.R. Part 99 (2013).\n2Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule, 45 C.F.R. Part 160 and Subparts A and E of Part\n164.\n313 U.S.C. §9. Information as confidential; exception.\n4Office of Management and Budget, Memorandum M-17-12. Preparing for and Responding to a Breach of Personally Identifiable\nInformation. January 3, 2017.\n5Regulation of the European Parliament and of the Council on the Protection of Individuals with Regard to the Processing of\nPersonal Data and on the Free Movement of Such Data, COM (2012) 11 final (Jan. 25, 2012).\nThe picture is even further complicated by the existence of gaps between normative\nconceptions of privacy in different cultures, and between disciplines. Gaps exist between these\nconceptions and their implementation in laws and regulations, and between the latter and\ntechnical conceptions of privacy. Describing and reconciling these gaps is a task of extremely\nlarge scope. For this reason, in this paper, we restrict our attention to the interactions between\ntechnical and normative aspects of privacy, focusing specifically on the setting of data analysis\nand on conceptions of informational privacy coming from the legal and computer science\nliteratures. Developing an understanding of the mechanics of the interactions between technical\nand normative aspects of privacy will be essential towards ensuring these various aspects of\nprivacy are in harmony. For example, it will be critical that legal conceptions of privacy be\ncognizant of what scientific knowledge deems deliverable. At the same time, technical definitions\nof privacy should provide a level of protection that meets societal needs and expectations.\n2. The data privacy problem\nThe subject of informational privacy is wide, encompassing a vast multitude of concepts.\nFor the purposes of providing a concrete illustration, this discussion focuses on a specific\nsubset of the privacy problem, namely an analysis of the differences between technical\napproaches for protecting privacy in statistical computation and the notions of de-identification\nand anonymization underlying many privacy regulations. This analysis yields a number of\nopportunities for overcoming the challenges created by the gaps between these varied concepts.\n(a) Privacy as a normative concept\nPrivacy is a normative concept deeply rooted in philosophical, legal, sociological, political and\neconomic traditions. Early principled discussion of privacy goes back to Aristotle’s distinction\nbetween public and private spheres of life [5]. An understanding of a vast range of privacy harms\nhas since been developed by the literature and addressed by legal frameworks [6]. This piece\nfocuses on the normative concepts regarding informational privacy embedded within various\nregulations and policies governing information privacy and data protection. Of relevance are\nregulations and policies restricting the release of statistical information about individuals or\ngroups of individuals, whether released as raw data, de-identified data or statistical summaries.\nThese include regulatory requirements for de-identifying or anonymizing information prior\nto disclosure found in laws and related guidance protecting education (see footnote 1) and\nhealth (see footnote 2) records in the USA, data provided by respondents to the US Census\nBureau (see footnote 3), personally identifiable information held by US federal agencies (see\nfootnote 4), and personal data about individuals in the European Union (see footnote 5), among\nmany others around the world.\n(i) Privacy torts\nAn early precursor to information privacy lawwas an 1890 essay byWarren & Brandeis titled ‘The\nRight to Privacy’ [7]. In this essay, Warren and Brandeis voiced concerns about the confluence\nof instantaneous photography and widespread newspaper circulation increasingly enabling\njournalists to intrude on private affairs. Characterizing privacy as the ‘right to be let alone’ and as\nan essential component of ‘the right to one’s personality,’ they invoked European philosophical\nand legal doctrine in articulating the right of an individual to develop his or her personality free\nfrom unwanted publicity [7]. They advocated a common law recognition of a right to privacy, an\nidea that has been highly influential, shaping the evolution of both common law and statutory\nlaw across the USA.\nIn 1960, Prosser published a summary of the subsequent jurisprudence on the right to privacy,\nfinding that a majority of American courts recognized the right. Furthermore, he found that\nprivacy law had evolved to comprise four distinct privacy torts, including intrusion upon a\nperson’s seclusion or solitude, or into his or her private affairs; public disclosure of embarrassing\nprivate facts about an individual; publicity placing one in a false light in the public eye; and\nappropriation of one’s likeness for the advantage of another [8].\nModern US courts continue to frame informational privacy interests in terms of the four\nprivacy torts, carrying forward Prosser’s emphasis on redressing specific, tangible harms\nresulting from a rather narrow subset of privacy-invasive activities. This narrow view of privacyrelated\nharms, requiring a showing of actual or imminent physical, financial or property injury,\nhas led US courts to dismiss many data breach cases for lack of harm. Even cases alleging that\nan individual’s sensitive information, such as his or her name and Social Security number, was\nbreached as a result of a company’s negligence have been dismissed for lack of injury [9]. These\ncases suggest that the privacy torts are ill-suited to address many types of modern data privacy\nrisks, particularly those associated with leakages of information from releases of statistics for\nwhich the possible injuries are unlikely to be actual or imminent. Moreover, tort law’s focus\non the harms that result from unauthorized access to information about an individual arguably\nfails to capture other categories of privacy harms, namely the accumulated leakage of personal\ninformation from data to which access was properly granted.\n(ii) Fair information practice principles\nIn 1973, the US Department of Health, Education and Welfare (HEW) published a report\ncontaining fair information practice principles for protecting personal data in record-keeping\nsystems, in response to concerns about the growing use of automated data systems [10]. This\nreport explicitly focuses on the protection of identifiable information. The principles set forth\nin this report govern the collection, use and storage of ‘personal data that can be associated\nwith identifiable information’ either by ‘specific identification, such as name or Social Security\nnumber’ or ‘because they include personal characteristics that make it possible to identify an\nindividual with reasonable certainty’ [10]. Based on a ‘concept of mutuality in record keeping,’\nthe report calls for safeguards enabling an individual ‘to find out what information about him is\nin a record and how it is used’, ‘to prevent information about him obtained for one purpose from\nbeing used or made available for other purposes without his consent’ and ‘to correct or amend a\nrecord of identifiable information about himself’ [10]. The principles also require an organization\nmaintaining identifiable personal data to ‘assure the reliability of the data for their intended use\nand . . . take reasonable precautions to preventmisuse of the data’ and hold that theremust be no\npersonal-data record-keeping systems ‘whose very existence is secret’ [10].\nIn addition, theHEWreport recognizes and provides a detailed discussion of the informational\nrisks to individuals associated with statistical data publications. In order to protect research\nrespondents, the report calls for new federal legislation to safeguard data ‘identifiable with, or\ntraceable to, specific individuals’ maintained by any statistical-reporting and research system.\nIt notes that, when releasing data in statistical form, an organization should address the risk of\n‘statistical disclosure,’meaning ‘the risk that arises when a population is so narrowly defined that\ntabulations are apt to produce cells small enough to permit the identification of individual data\nsubjects, or when a person using a statistical file has access to information which, if added to data\nin the statistical file, makes it possible to identify individual data subjects’ [10].\nCongress based provisions of the Privacy Act of 19746 in large part on the findings of this\nreport, thereby applying the fair information practice principles to all US federal agencies. Similar\nstatutory requirements have been enacted at the state level to govern the practices of US state\nagencies.7 International privacy guidelines based on the fair information practice principles\nhave also been adopted by governments around the world, such as the European Union.8\nThe most widely followed guidelines are the privacy principles developed by the Organisation\n6Privacy Act of 1974, 5 U.S.C. §552a.\n7E.g., Mass. Gen. Laws ch. 66A; Minn. Stat. §13.01 et seq.\n8Directive 95/46/EC of the European Parliament and of the Council of 24 October 1995 on the protection of individuals with\nregard to the processing of personal data and on the free movement of such data (Data Protection Directive).\nfor Economic Co-operation and Development (OECD), which encompass the broadly defined\nprinciples of collection limitation, data quality, purpose specification, use limitation, security\nsafeguards, openness, individual participation and accountability [11]. These principles apply\nonly to ‘information relating to identified or identifiable individuals’ and explicitly exclude\n‘anonymous data,’ such as ‘collections of statistical data in anonymous form’ [11].\nAlthough strongly influenced by the 1973HEWreport, which provides an extended discussion\nof privacy risks and guidance on how to balance risks against uses of data, the laws and\ninternational guidelines that followed are significantly less detailed. For instance, because the\n‘precise dividing line’ between identifiable and anonymous data ‘may be difficult to draw,’ the\nOECD principles leave further guidance on this issue to individual regulations at the national\nlevel [11]. More generally, critics have noted that the fair information practice principles are not\nself-implementing or self-enforcing, with actual implementation taking place at the statutory,\nregulatory, or organizational level [12]. Implementation of the principles is highly sector-specific\nand context-dependent and, further, ‘in any context is often more a matter of art and judgement\nrather than a science or mechanical translation of principles’ [12].\n(iii) De-identification\nThe fair information practice principles—and the statutes, regulations and policies implementing\nthem—centre on concepts such as identifiable data and anonymous data. These concepts are\ngrounded, in large part, in the longstanding practice of using an identifier to retrieve an\nindividual’s record from an administrative record-keeping system (see [10]). Organizations\nhave an obligation to protect this information, as it can potentially be used to access sensitive\ninformation about an individual from a database. For instance, federal statistical agencies\nhave established a wide range of safeguards to ensure their statistical data releases do not\ncontain identifiers that could be used to locate an individual’s record in an administrative\nsystem (see [10]).\nSince then, notions of identifying information have continued to play a central role in\ninformation privacy laws enacted in jurisdictions around the world. Although they vary\nsignificantly with respect to the types of information protected and the safeguards required, they\noften turn on a specific definition of personally identifiable information, personal information or\npersonal data [13]. If information falls within a particular law’s definition of personal information,\nit is generally protected from disclosure. Definitions of personal information differ considerably\nacross sectors, jurisdictions and contexts. Some regulations offer narrow definitions, such as the\nMassachusetts data security regulation, which defines personal information as a Massachusetts\nresident’s name in combination with his or her Social Security number, driver’s license number,\nor financial account number.9 Other regulations are considerably broader, such as the EU General\nData Protection Regulation, which defines personal data as ‘any information relating to a data\nsubject’ (see footnote 5).\nSome information privacy laws expressly exclude information classified as de-identified\nor anonymous data from the definition of personal information. Information that has been\ntransformed in accordance with regulatory requirements for de-identification can generally be\nshared more widely, or, in some cases, can even be disclosed publicly without further restriction\non use or redisclosure. For example, the HIPAA Privacy Rule provides a safe harbour that\npermits the disclosure of health information that has been de-identified through the removal\nof information from a list of 18 identifiers, such as names, Social Security numbers and dates\nof birth.10 Other regulations may require case-by-case determinations to be made when deidentifying\ninformation. For example, regulators have declined to specify whether a particular\nset of methods for de-identifying information is sufficient to meet the requirements of FERPA\nand, instead, instruct educational agencies and institutions to make a determination based on the\n9201 Code Mass. Regs. §17.02.\n1045 C.F.R. §164.514.\ndataset itself, other data sources that may be available and other context-specific factors.11 The\nopen-ended, contextual nature of de-identification standards poses challenges for data holders\nand other practitioners who often may not know with certainty whether they have adequately\nsatisfied the de-identification requirements of the applicable laws.\nMoreover, definitions of personal information are evolving over time in response to expanded\nnotions of what information could be used to identify an individual via a privacy attack on a\ndatabase. For instance, in 2017, the US Office of Management and Budget updated its guidance\nto federal agencies on preparing for and responding to a breach of personally identifiable\ninformation. The new definition of personally identifiable information advises that ‘information\nthat is not PII can become PII whenever additional information becomes available—in any\nmedium or from any source—that would make it possible to identify an individual’ (see footnote\n4). Differences between regulatory definitions, uncertainty regarding their scope, and changes in\nhow they are defined and interpreted in light of the broader data privacy landscape are widely\ncited as weaknesses of the regulatory framework for privacy protection [13].\n(iv) Contextual integrity\nThere is growing recognition that traditional conceptions of privacy fail to accurately capture\nnormative expectations of privacy. A highly influential alternative framework is contextual\nintegrity, introduced by Nissenbaum [14] and incorporated in legislative proposals such as the\nConsumer Privacy Bill of Rights Act of 2015 [15]. This framework rejects notions of privacy as\ncontrol over information about oneself or as a strict dichotomy between public versus private\ninformation or sensitive versus non-sensitive information. Privacy is, instead, best understood in\nterms of normative expectations about the appropriate flow of information.\nContextual integrity assesses how closely the flow of personal information conforms to\ncontext-relative informational norms. More precisely, in a context, the flow of information of a\ncertain type about a subject from a sender to a recipient is governed by a particular transmission\nprinciple. Contextual integrity is violated when the norms in the relevant context are breached.\nIntuitively, it recognizes that certain parties may obtain certain types of information about other\nparties under the right terms and for the right reasons. For example, a patient (subject and\nsender) may communicate concerns about his or her medical condition (information type) to\nher physician (recipient) during an annual medical examination, with the understanding that\nthe patient–physician relationship demands confidentiality (transmission principle). A physician\nsharing details about the patient’s medical condition with the patient’s insurer would not\nconstitute a violation of contextual integrity, whereas sharing the same information with the\npatient’s employer would.\nAs an ethical justificatory framework for socio-technical systems, contextual integrity seeks to\ntake into consideration the individual interests and preferences of affected parties, the ethical\nand political principles and values of societal distribution, and societal contextual functions,\npurposes and values. It recognizes that contexts evolve over time in cultures and societies, subject\nto historical, cultural, geographical factors and aims to capture how normative expectations\ndepend on such contextual factors. An active area of research has sought to use this framework\nto disentangle various justifications for normative theories and legal conceptions of privacy [14].\nExtensions of this research have also explored technical applications, such as the design of access\ncontrol and privacy policies based on a formalization of privacy law requirements using the\ncontextual integrity framework [16].\n(b) Privacy as a technical concept\nIn addition to being a normative concept, privacy is a technical concept underlying the design of\nprivacy-preserving technologies. Such technologies draw from a range of definitions of privacy\n11See Family Education Rights and Privacy, 73 Fed. Reg. 74,805, 74,853 (Dec. 9, 2008).\npresented in the statistics and information security literature. As discussed below, these technical\nconcepts often disagree in fundamental ways about what it means to protect individual privacy.\n(i) Anonymization and de-identification\nMany privacy technologies are designed with the goal of de-identifying personal information.\nThis approach equates privacy protection with making personal information anonymous or deidentified,\ni.e. preventing an individual’s information from being linked with him or her. The\npremise is that it is impossible (or, at least, very difficult) to infer personal information pertaining\nto an individual from a de-identified dataset or use it to violate an individual’s privacy in\nother ways.\nThe process of de-identifying data typically involves a combination of data redaction and\ncoarsening techniques. Examples include suppression of directly identifying attributes (e.g.\nnames, addresses and identification numbers); suppression of indirectly identifying attributes,\ni.e. those that can be linked to external data sources that contain identifying information (e.g.\nthe combination of ZIP code, sex and date of birth); generalization or coarsening of data (e.g.\nby grouping ages into ranges or grouping geographical locations with small populations); and\naggregating and perturbing data (e.g. [17,18]).\nConcerns about the efficacy of de-identification techniques have been raised in the technical\nliterature on privacy since the late 1990s. Researchers have repeatedly demonstrated that it is\npossible to re-identify data believed to be de-identified. In many cases, re-identification has\nbeen accomplished via successful linkage attacks, whereby a de-identified dataset is joined\nwith a publicly available dataset containing identifying information. Any record in the deidentified\ndataset which is uniquely linked with a record in the publicly available dataset (using\nthe attributes common to both) is re-identified. For example, Sweeney demonstrated that deidentified\nhospital records could be joined with voter registration records, as both datasets\ncontained birth date, sex and ZIP code information for each individual. Sweeney used these\nthree attributes to uniquely identify Massachusetts Governor William Weld’s health records and\nobserved that the process she used resulted in many other uniquely identified records.12 More\ngenerally, she showed that a large portion of the US population could be uniquely re-identified\ngiven just these three pieces of their information [19]. Subsequent attacks on de-identified data\n(e.g. [20,21]) suggest that as few as three or four data points can be sufficient to re-identify an\nindividual in a de-identified dataset. In fact, to be effective, de-identification must strip the data\nof most of its informational content, rendering it almost void of value of analytic purposes (see\ndiscussion in [22,23]). There have been many attempts to define the concept of de-identification\nheuristically (e.g. k-anonymity [24] and its variants l-diversity [25] and t-closeness [26]). However,\nas researchers continually discover new privacy vulnerabilities, new heuristic definitions must\nbe developed to address them. In practice, heuristic approaches can be used to protect personal\ninformation against a small number of specific attacks, but they do not provide comprehensive\nprotection against all feasible attacks.\nTo date, there is no formal mathematical definition for de-identification, or for related\nconcepts such as personally identifiable information and linkage. Because these concepts are\nheuristic rather than formal, they must be periodically updated in response to newly discovered\nweaknesses. Privacy standards based on these heuristic concepts becomemoving targets, creating\nuncertainty for practitioners (i.e. whether the techniques they use provide sufficient privacy\nprotection) and for the individuals in the data (i.e. whether and when data publications expose\nthem to risks). Note that although the heuristic approaches mentioned above have been defined\nusing mathematical language, they are syntactic in nature (i.e. specifying properties of how an\nanonymized dataset should look) rather than semantic (i.e. specifying restrictions on what an\n12Recommendations to Identify and Combat Privacy Problems in the Commonwealth: Hearing on H.R. 351 Before the House\nSelect Committee on Information Security, 189th Sess. (Pa. 2005) (statement of Latanya Sweeney, Associate Professor, Carnegie\nMellon University).\nattacker may infer about the personal information that is the input for the anonymization process\nby observing its outcome). Consequently, these notions do not provide a precise understanding\nof the privacy protection provided, such as how the potential harm to an individual may be\naffected by being included in a k-anonymized, l-diverse or t-close dataset. The lack of formal\nunderstanding limits the scope of scientific discussion of de-identification to, roughly, producing\na collection of techniques, the efficacy of which is difficult or even impossible to measure\nand compare.\n(ii) Semantic security\nThe foregoing discussion refers to the advantages of semantic over syntactic definitions of privacy.\nA key example of a semantic definition is semantic security, a definition that was introduced by\nGoldwasser & Micali [27]. Semantic security is a standard privacy requirement of encryption\nschemes. To understand what it requires, consider a scenario in which Alice uses a public-key\nencryption scheme to communicate a confidential message m to Bob. She encrypts the message\nusing Bob’s encryption key and sends him the resulting ciphertext c. Using his (secret) decryption\nkey, Bob can then recover the message m from the ciphertext c. The definition of semantic\nsecurity compares what an attacker (without access to Bob’s decryption key) can predict about\nthe message m given the ciphertext c with what the attacker can predict about the message m\nwithout being given the ciphertext c. The advantage that access to the ciphertext gives to any\nattacker is quantified. Encryption schemes are designed to make this advantage so negligible that\naccess to the ciphertext does not give the attacker any practical advantage over not getting any\ninformation about the message m at all.\nAn early influential work on data privacy by the statistician Tore Dalenius defined disclosure\nas follows. ‘If the release of the statistics S makes it possible to determine the value Dk more\naccurately than is possible without access to S, a disclosure has taken place’ (here Dk refers to\nthe personal information of subject k) [28]. This view of disclosure is interpreted as having a\ngoal similar to semantic security: ‘access to a statistical database should not enable one to learn\nanything about an individual that could not be learned without access’ [29].13\nNote that, while not fully formalized, this desiderata is essentially equivalent to semantic\nsecurity. There are, however, significant differences between applying this concept to encryption\nand to data privacy. In particular, the setting of encryption schemes clearly distinguishes between\nthe party who should be able to learn the message m (i.e. Bob) and an eavesdropping attacker\nwho should not gain any information about m. By contrast, when statistics are computed over\na collection of personal data, the analyst (understood broadly as any party with access to the\npublished outcome) is both the proper consumer of the statistics and a potential privacy attacker.\nSemantic security has proved to be a fundamental concept for encryption. In fact, encryption\nschemes that are in common use today are semantically secure (under some mathematical\nassumptions). However, Dwork & Naor [29] demonstrated that the concept cannot be applied\nin the context of private data analysis as it would imply (reusing Dalenius’ words in [28])\n‘elimination of statistics’. To understand what this means, consider an example, taken from\nDwork & Naor [29]. Suppose the attacker has the auxiliary information ‘Terry Gross is two\ninches shorter than the average Lithuanian woman’. Without access to statistical information\nabout Lithuanian women, the attacker has little information about Terry Gross’ (secret) height,\nbut the situation changes dramatically once statistics about the average heights of the Lithuanian\npopulation are published, as the attacker can now combine the published average height with\nhis auxiliary information to learn Gross’ height. The release of the average height of Lithuanian\nwomen (whether exact or an approximation) does not satisfy semantic security as it allows some\nattackers (those who posses relevant auxiliary knowledge) to improve their predictions of Gross’\n13Dalenius recognizes that ‘[a] reasonable starting point is to discard the notion of elimination of disclosure’ as ‘it may be\nargued that elimination of disclosure is only possible by elimination of statistics’ [28]. The argument by Dwork and Naor still\nholds even if their interpretation is relaxed to allowing an attacker to obtain a small advantage in predicting private personal\ninformation given the access to the statistical database.\nheight. This illustrates how the semantic security desiderata fails to address access to auxiliary\ninformation, i.e. information available to an attacker from outside the system.\nA second example illustrates how the semantic security desiderata fails to differentiate\nbetween information about a person and information that is specific to a person (i.e. information\nthat cannot be inferred unless his or her information were used in the analysis). Suppose a\nresearch study explores the effects of wine drinking in 60-year-old women and finds a strong\npositive correlation between wine drinking and kidney disease thatwas not known to exist before\nthe study was conducted. If one knows that Gertrude is a 60-year-old woman and that she is a\nregular wine drinker, the results of the study can be used to better estimate whether Gertrude\nsuffers from kidney disease than before the study was conducted. The release of the study’s\nresults, therefore, does not satisfy semantic security. Further research extends these examples and\ngeneralizes them in a formal mathematical argument thatmaking semantic security a requirement\nin data analysis would essentially prohibit the discovery of any new knowledge through research\nusing personal data [29].\nThe discussion above should not be understood to undermine the importance of cryptographic\nnotions such as semantic security to privacy. In fact, modern cryptographic methods such as\nsecure multiparty computation [30] and homomorphic encryption [31] extend the notion of\nsemantic security to settings in which several parties (each holding a private input) compute a\nfunction f () of their joint data without having to reveal any information beyond what is revealed\nat the conclusion of the following ‘ideal’ (imaginary) process: a fully trusted party collects the data\nfrom all the parties, computes f (), informs each party of her designated part of f (), and erases the\nentire process fromits memory. Such cryptographic approaches are now beginning to be deployed\nin real-world settings, such as personalized medicine [32].\n(iii) Formal privacy models and differential privacy\nFailures of traditional privacy-preserving approaches to control disclosure risks in statistical\npublications have motivated computer scientists to develop a strong, formal approach to privacy.\nThe formal study of privacy has grown successfully out of the study of cryptography, beginning\nwith the seminal work of Diffie & Hellman [33]. This work helped to establish rigorous\nmathematical foundations for the field of cryptography. One of the benefits of this approach\nwas an ability to avoid the ‘penetrate-and-patch’ dynamics of older cryptographic schemes\nwhose design had to be repeatedly modified to counter newly found attacks. New cryptographic\nschemes could be designed to be provably resilient to any feasible adversarial attack. For this\nreason, it is now extremely rare that a security system is vulnerable because of a weakness in a\ncryptographic algorithm (though vulnerabilities may nevertheless exist in the implementation of\nthe algorithm or in other components of the system).\nThe formal approach to privacy includes both a study of what cannot be computed with any\nreasonable notion of privacy and what can be computed with strong notions of privacy, with\nthe goal of closing the gaps between the two. At the centre of this approach stands a collection\nof formal concepts defined with mathematical precision. Furthermore, statements about these\nconcepts are provedmathematically (rather than, say, empirically). For example, research seeks to\ndevelop a formal notion of what should be considered non-private under any reasonable notion of\nprivacy. This very minimal view of privacy, called a reconstruction attack, corresponds to a setting\nin which a privacy attacker can use the database’s purportedly privacy-preserving mechanism\nto accurately reconstruct the database. Dinur & Nissim [34] used reconstruction attacks to prove\na lower bound on the magnitude of noise that is essential for preserving privacy in the context\nof a database answering multiple statistical queries. In this setting, each of the statistical queries\nseems to leak very little personal information; however, the accumulated leakage amounts to a\nmassive loss of privacy. This phenomenon was shown to hold in other settings, indicating that if\ntoo many statistics are released with too high accuracy then privacy is lost.\nCentral to the current study of formal privacy models is the notion of differential privacy,\nintroduced by Dwork, Mcsherry, Nissim & Smith [35]. Differential privacy is a formal\nmathematical standard for quantifying and managing privacy risk. The definition requires the\noutput distribution of a privacy-preserving analysis to remain ‘stable’ under any possible change\nto a single individual’s information. Differential privacy has a compelling intuitive interpretation\nas it guarantees to every individual that the consequences to her would be similar regardless\nof whether her information were used in the analysis or not. We refer the reader to [36] for a\nmore detailed discussion of the protection provided by differential privacy and to [35] for a more\ntechnical presentation of the mathematical definition and its properties.\nIt is worth emphasizing that differential privacy is not a specific tool or technique for privacy\nprotection but a definition or standard for quantifying and managing privacy risks. A range of\ntechnological tools can be devised to satisfy the differential privacy standard. It differs from all\nnotions of anonymization mentioned above in that it restricts the informational relationships\nbetween the personal information that is used as input to a privacy-preserving analysis and\nthe outcome of the privacy-preserving analysis. The excess risk to an individual resulting\nfrom her participation in a differentially private analysis is bounded (or strictly controlled),\nwhereas such a guarantee is impossible for the anonymity concepts. Another key property of\ndifferential privacy is that it self-composes. Any mechanism resulting from the composition of\ntwo or more differentially private mechanisms is also differentially private (albeit, with worse\nparameters). Composition is a property that prior attempts to define privacy lacked. Currently,\ndifferential privacy is also the only framework giving meaningful privacy guarantees in the face\nof adversaries having access to arbitrary external information. Furthermore, analyses satisfying\ndifferential privacy provide provable privacy protection against any feasible adversarial attack,\nwhereas the anonymity concepts only counter a limited set of specific attacks.\nComparing differential privacy with semantic security, the crucial difference is that semantic\nsecurity compares what an attacker can infer about an individual with and without access to the\nstatistics disclosed, whereas differential privacy compares what an attacker that has access to the\nstatistics can infer about an individual whether her information is used in computing the statistics\nor not. Recall that the analysis of Dwork & Naor [29] showed that semantic security entails no\nutility in data analysis. In sharp contrast, there is a continually growing list of tasks that has\nbeen shown, in principle, to be computable with differential privacy, including descriptive and\ninferential statistics, machine learning algorithms and production of synthetic data. Existing realworld\napplications of differentially private analyses include implementations by federal agencies\nsuch as the US Census Bureau and companies such as Google, Apple and Uber.\nIn settings in which one seeks to analyse data that are distributed among several agencies\nbut cannot be shared among themselves or with a data curator, differential privacy may\nbe implemented with cryptographic techniques, such as secure multiparty computation, for\ncomputing over the distributed data. In some cases, integrating differential privacy with\ncryptographic techniques can yield greater accuracy at the same level of privacy.\n(c) Gaps between normative and technical concepts\nThere are substantial gaps between technical and normative conceptions of privacy, and these\ngaps are magnified by the emergence of formal privacy models. Chiefly, formal privacy models\nsuch as differential privacy are defined with mathematical precision, and statements about\nwhether their requirements are satisfied can be proved rigorously. Uncertainty with respect to the\ncorrectness of such statements is eliminated once their proofs are verified. By contrast, normative\napproaches to privacy are inherently flexible and subject to interpretation. Assessing privacy with\nrespect to normative expectations can lead to varying—and even contradictory—conclusions.\nPractical guidance on interpreting normative concepts often, by design, leaves the boundaries\nbetween what is private and what is non-private not fully determined. On the one hand, this\nallows interpretation of the standard to evolve, and, on the other hand, creates uncertainty.\nThe following discussion illustrates a number of the gaps created by these differences, through\na comparison of formal privacy models like differential privacy to the normative approaches\nreflected in information privacy laws.\n(i) Generality of protection afforded\nRegulatory requirements for privacy protection vary according to industry sector, jurisdiction,\ninstitution, types of information involved or other contextual factors [13]. For example, the\nFamily Educational Rights and Privacy Act protects only certain types of information contained\nin education records maintained by schools, universities and educational agencies. However,\nin practice, privacy risks are not limited solely to the information categories and contexts\ncontemplated in the law. Furthermore, interpreting and applying regulatory standards is\nchallenging in cases in which an analyst seeks to combine data from multiple sources. In contrast\nwith regulatory requirements, a formal privacy model like differential privacy offers general\nprotection and can, in principle, be applied wherever statistical or machine learning analysis is\nperformed on collections of personal information, regardless of the contextual factors at play. In\nrecognition of the fact that a broader notion of personal information must be protected in order\nto preserve privacy and that information that currently seems innocuous may prove sensitive\nin the future, formal privacy models protect all information specific to an individual, not just\ninformation traditionally considered to be identifiable or able to be linked to an individual.\n(ii) Scope of attacks contemplated\nPrivacy regulations and related guidance contemplate a limited set of specific attacks and\nprivacy failure modes. As one example, many regulations make an implicit assumption that\nre-identification via record linkage—i.e. the re-identification of one or more records in a deidentified\ndataset by uniquely linking these records with identified records in a publicly available\ndataset—is the primary or sole privacy failure mode. Other central concepts appearing in\nprivacy regulations, including personally identifiable information, (de-)identification, linkage\nand inference, are often defined from this point of view. For example, many privacy regulations\nrequire data providers to protect information that can be linked to an individual in order to\nsafeguard against record linkage. As a result, these requirements are often interpreted as requiring\nthe protection of information one can foresee being used in a record linkage attack.\nHowever, in the last two decades, researchers have identified new attacks and privacy failure\nmodes. In some cases, learning whether an individual participated in a research study could be\nconsidered a privacy violation, even if the individual’s exact information cannot be identified.\nFor instance, if an employer learns that a job candidate previously participated in a research\nstudy investigating the efficacy of interventions for substance abuse, he or she may infer that\nthe candidate has a history of substance abuse, even without identifying the candidate’s record\nin the database. Privacy risks can take other forms as well, such as singling out an individual\n(even if not fully identified), or inferring information that is specific to an individual with less\nthan absolute certainty. Privacy regulations that focus on re-identification via record leakage can\nfail to address this broader understanding of the potential modes of privacy failure.\nIn contrast with existing privacy regulations, formal privacy models provide protection against\na wide collection of privacy attacks, even those that are not currently known. More specifically,\nformal models focus on provable limitations to excess harm due to participation in a computation,\nregardless of the failure mode that occurred.\n(iii) Expectations versus the scientific understanding\nRegulatory standards that rely on the concept of de-identification to protect privacy are often\nnot in agreement with the current scientific understanding of privacy. For example, the HIPAA\nPrivacy Rule allows the publication of personal health records, as long as certain pieces of\ninformation deemed to be identifying have been removed (see footnote 11). This is now\nunderstood to be a weak standard, as research has demonstrated that redaction of identifiers\ncan fail to protect privacy, especially when applied to information that is very detailed, such as\nthat found in medical records. In fact, any information about individuals, including information\nnot traditionally considered to be identifying, has the potential to leak information specific to\nindividuals [23]. Moreover, this issue is not limited to HIPAA, as many legal standards of privacy\nrely on the concepts of de-identification and personally identifiable information.\nIn some cases, the law may be interpreted to require something that is not technically feasible,\nsuch as absolute privacy protection when sharing personal data. For example, Title 13 of the\nUS Code protects the confidentiality of respondent information collected by the US Census\nBureau by prohibiting ‘any publication whereby the data furnished by. . .[an] individual. . .can\nbe identified’ (see footnote 3). Whether an individual can be identified in a publication is not\nprecisely defined. If this concept were interpreted very conservatively, Title 13 would disallow\nany leakage of information about individuals. This, in turn, would prohibit the Census Bureau\nfrom publishing any statistics based on data furnished by individuals, as every release of statistics\nunavoidably implies some level of privacy loss for the individuals whose information has\nbeen analysed.\nThe binary view of privacy found in Title 13—whereby information is either identifiable or\nnot—is common to many regulations. Such an approach is problematic (i) because information\ncan never be made completely non-identifiable and (ii) because it fails to recognize that privacy\nloss accumulates with successive releases of information about the same individuals. Not only\nis some privacy loss inevitable in every release of statistics, but these leakages accumulate\nand can eventually amount to a significant disclosure of personal information. Formal privacy\nmodels such as differential privacy bound the privacy leakage of each release, and furthermore\nare equipped with a host of tools (called ‘composition theorems’) that bound the total privacy\nleakage across multiple releases. However, composition, a key feature of formal privacy models,\nis generally ignored by regulatory standards for privacy protection.\n(iv) (In)-stability over time\nNotions of privacy embedded within regulatory standards are continually evolving in\nresponse to new discoveries of vulnerabilities. Practitioners seeking to implement privacy\nsafeguards in accordance with these regulations, therefore, face a moving target. As one\nexample, the US Office of Management and Budget’s guidance on protecting personally\nidentifiable information has been updated over time to address an evolving understanding\nof the ways in which de-identified data may be vulnerable to potential attacks. The latest\nupdate to the guidance advises government agencies that they must consider that nonpersonally\nidentifiable information may become personally identifiable information in the\nfuture (see footnote 4). This approach to defining the scope of information to be protected\nrequires regulatory and policy standards to be updated as new attacks are identified, much\nlike the ‘penetrate-and-patch’ approach to patching software incrementally as new bugs\nare discovered.\nHowever, when a regulatory definition of privacy is periodically amended over time, it is an\nindication that the definition is not a strong, general definition of privacy. Rather, the definition\nreflects a much narrower presumption that certain approaches are likely sufficient to provide\nadequate privacy protection. Regulations such as the HIPAA Privacy Rule (see footnote 11)\nthat require the removal of certain pieces of identifying information have hardwired a specific\ntechnique—redaction of identifiers—into their standards. Over time, as techniques such as\nredaction are shown to be inadequate to protect privacy, practitioners are left with uncertainty\nregarding what is required to satisfy regulatory standards that are out of step with best practice.\nBy contrast, a formalmodel like differential privacy is the subject of ongoing scientific research,\nregardless of implementation. This research provides a strong assurance that differential privacy\nprovides a sufficient level of privacy in an extremely wide collection of settings. This assurance is\nsupported by mathematical theory providing provable privacy guarantees for any combination\nof differentially private analyses, provided they are correctly implemented.14\n14We note that variants of differential privacy are the subject of research. The goal of these variants is to provide improved\naccuracy while providing provable privacy guarantees similar to those of differential privacy.\n(v) Relationship to normative expectations\nOn first impression, the relationship between some technical and normative concepts of privacy\nmay appear to be straightforward. For example, it may seem to follow intuitively that differential\nprivacy satisfies the requirements of many regulatory requirements, as well as many of the\nnormative expectations underlying such requirements. This is because, inmany cases, differential\nprivacy provides protection that is more robust than that provided by traditional statistical\ndisclosure limitation techniques commonly used to satisfy regulatory requirements for privacy\nprotection. Moreover, because they are founded on rigorous mathematical grounds, formal\nprivacy models provide protection against a wide range of potential privacy attacks, including\nattacks that have succeeded against traditional techniques.\nWe observe, however, that there are significant conceptual gaps between formal privacy\nmodels and normative standards and expectations of privacy. We caution that an analysis\ncomparing the protection afforded by a particular privacy technology to a normative standard\nmust be done with care. For illustration, we provide a few examples demonstrating why making\na sufficiency claim with respect to differential privacy and legal requirements for privacy is a\nnon-trivial task.\nFirst, formal privacy models are, by and large, ‘privacy-first’ definitions. These definitions\nmake the privacy desiderata primary, and they subject other desiderata, such as accuracy,\ncomputational costs and sample complexity, to the limitations implied by the privacy guarantee.\nIn many practical settings, uses of data may demand a compromise between protecting privacy\nand carrying out accurate computations on the data, and finding the right balance can be\nchallenging. For instance, some existing real-world applications of differential privacy have been\nimplemented with parameters thatwere selected to improve accuracy but, in principle, may allow\nfor a significant leakage of personal information [37].\nSecond, some regulatory standards express requirements for privacy protection that can be\ninterpreted to exceed the protection provided by various privacy technologies, including formal\nprivacy models like differential privacy. For example, the US Census Bureau has an obligation\nto protect the privacy of both individual respondents and establishments. Haney et al. observed\nthat, while differential privacy is a notion suitable for the protection of information pertaining\nto individuals or small groups of individuals, such as a family, it does not necessarily protect\nestablishments [38]. Their research offers suggestions for modifying differential privacy with\nadditional requirements to prevent precise inferences of establishment size and the composition\nof the establishment workforce [38]. This real-world case illustrates how normative requirements,\nsuch as those embodied in the law, can direct the development and implementation of formal\nprivacy models.\nLastly, as we discussed above, differential privacy protects information that is specific to a\ndata subject—i.e. information that can only be inferred about the subject if his or her information\nis used in the analysis. However, as discussed earlier in this section, some regulations can be\ninterpreted to require the protection of data that is not specific to an individual. This poses a\nresearch direction for the formal mathematical study of privacy: to understand formally which\npart of this expectation can be met with a rigorous mathematical definition, and the implications\nof such a definition on the analyses performed in a variety of tasks, including both research- and\ncommercially focused analyses.\n3. A way forward: hybrid concepts of privacy\nAdopting an understanding of privacy that is consistent across its technical and normative\ndimensions will be critical to ensuring personal data are adequately safeguarded over the long\nterm. However, significant conceptual gaps between existing technical and normative concepts\ncreate challenges for arriving at a universal notion of privacy. For instance, normative concepts\nembedded in existing regulatory requirements for privacy protection often rely on intuitive\nassumptions about how pieces of information interact, rather than (and often contradicting)\nscientific and mathematical principles. Framing privacy in this way can result in expressions of\nunrealistic privacy desiderata, leading practitioners to pursue an idealized privacy goal that is\nimpossible to achieve. At the same time, purely technical approaches may adopt a narrow view\nof privacy that fails to capture the fundamental normative expectations of privacy.\nBridging these gaps will be necessary to ensure robust privacy protection in practice. A\nsignificant first step is to recognize that privacy concepts have a hybrid nature. They are\nneither purely legal nor purely technical, but rather a multi-dimensional combination of the two.\nUnderstanding the hybrid nature of these concepts and developing tools for implementing them\nis necessary to bridge the gaps between the current legal and technical understanding of privacy.\n(a) Contextual integrity\nNissenbaum’s seminal framework—contextual integrity [14]—is an approach that aims to capture\nthe dual technical–normative nature of privacy. Contextual integrity is a justificatory framework\nfor privacy. It states that privacy breaches can be tracked to violations of societal norms regarding\nappropriate information flows within a particular context. Where it is possible to encode norms\nformally and determine whether a particular information flow respects these norms, i.e. when the\nnorms are unambiguous and furthermore effectively testable, then the framework can be used to\nprecisely predict or flag violations of the societal norms. As an example, Barth et al. [16] provide\na model for encoding norms appearing in legal standards such as HIPAA.15\nNormative concepts are often not defined explicitly, and,when they are, they are not expressed\nin a formal language that enables a precise analysis. As a result, there is uncertainty with respect\nto which information flows are in agreement with normative concepts. In many cases, it may\nbe difficult to determine with reasonable certainty whether an information flow is appropriate,\nwhether it creates a risk of a privacy breach or even whether a privacy breach has in fact occurred.\nIn order to bridge normative and technical concepts, the framework of contextual integrity\ncould in the future be equipped with formal mathematical definitions. If defined formally, its\nfundamental concepts, i.e. context, information flow and norm, could provide an interface for\nintegrating formal privacy models into the framework. As an example, the formalization of\ncontext, information flow and norm would allow researchers and policymakers to reason about\nthe appropriateness of using differentially private analyses in various contexts. This reasoning\nwould involve making and proving quantifiable statements regarding the extent to which\ndifferential privacy respects social norms around privacy in different contexts.\n(b) Bridging the legal and technical perspectives\nMotivated by the lack of translational work to bridge the legal and technical perspectives of\nprivacy, we, together with our colleagues, have sought in prior work to develop an approach to\ndemonstrating that a particular privacy technology can be used to satisfy a regulatory standard\nfor privacy protection. As an example, we applied the proposed approach to demonstrate\nthat differential privacy can be used to satisfy the de-identification requirements of the Family\nEducational Rights and Privacy Act (FERPA). This approach involved identifying a description\nof a potential privacy attacker and the attacker’s goals, which were embedded within FERPA’s\ndefinition of personally identifiable information.With a legal analysis of this regulatory language,\nwe developed a detailed description of the potential attacker contemplated by FERPA. We\nthen developed a mathematical model of FERPA’s privacy requirements and made conservative\nassumptions in the model with the goal of accounting for possible ambiguities in the regulatory\nstandard. Finally, we analysed differential privacy with respect to the mathematical model\nextracted from FERPA’s privacy requirements [39].\nEfforts to extend this work to other laws, such as the law protecting the confidentiality of\ninformation the US Census Bureau collects from respondents, led to the observation that other\n15Technically, this model is based on first-order logic (allowing predicates as well as quantifying over variables) extended\nwith temporal operators that allow expressing conditions on the timing relationships between events.\nlaws do not describe a potential attacker or the attacker’s knowledge in the way that FERPA does.\nThis makes it difficult to generalize the specific approach that was used to model FERPA. Despite\nthis challenge, there is an alternative approach that may provide a solution to the modelling\nproblem. This approach involves identifying fundamental concepts used in regulatory standards\nfor privacy protection and the statistical disclosure limitation literature and performing a detailed\nlegal analysis of these concepts. Based on this analysis, one canmodel the conceptmathematically,\nand then check whether the modelling agrees with the legal analysis. If it does, then one can\nproceed by comparing the mathematical model of the concept to a technical definition of privacy\nin order to demonstrate whether the technical definition meets the definition of the privacy\nconcept extracted from the regulation or literature.We suggest that such an approach can be used\nto design hybrid concepts of privacy, i.e. concepts that can be interpreted and used consistently\ntechnically and normatively.\n(c) Future regulation\nDeveloping an understanding of the gaps between technical and normative approaches to privacy\ncan also help identify ways in which to improve future privacy regulations. We argue that future\nregulations should aim to articulate clear goals for privacy protection that are in line with the\nscientific understanding of privacy, rather than implicitly or explicitly endorsing heuristic deidentification\ntechniques. As an example of a recent attempt to bring greater clarity to regulatory\nrequirements by explaining the goals of privacy protection, consider the EU’s General Data\nProtection Regulation, which protects personal data but not anonymous data (see footnote 5). The\nregulation does not state a clear goal, making it difficult to interpret what exactly it is intended\nto protect. However, guidance on interpreting the regulation outlines goals that go beyond the\ntraditional notion of de-identification, namely protection from singling out, linking, or inferring\nan individual’s personal data froma dataset.16 Although these privacy concepts have not yet been\ndefined precisely and formally from a mathematical perspective, they aim to describe what the\ngoal of a privacy-preserving mechanism should be, rather than prescribing a specific family of\ntechniques.\n(d) Bridging legal and other normative perspectives\nIt is also important to acknowledge that the normative concepts embedded within privacy\nregulations may not be a perfect mirror of more fundamental normative concepts of privacy.\nFor instance, the choices made in the design of privacy regulations may reflect expedient political\nor practical compromises, rather than actual individual and societal expectations. It may in fact\nnot be possible to close the gap between legal and other more fundamental normative concepts.\nHowever, by learning how to analyse whether a technology satisfies a legal definition, it may also\nbe possible to develop analogous approaches for analysing whether a technology satisfies other\nnormative expectations of privacy, including those that serve as the motivation for regulatory\nstandards for privacy protection.\nData accessibility. This article has no additional data.\nAuthors’ contributions. This work was conceived and developed jointly and equally by the authors.\nCompeting interests. The authors declare that they have no competing interests.\nFunding. The authors are supported by theUS Census Bureau under cooperative agreement no. CB16ADR0160001.\nAcknowledgements. The authors thank Simson Garfinkel and the editor and reviewers of Philosophical\nTransactions A for their helpful comments.\nReferences\n1. Cohen JM. 2013 What privacy is for. Harv. Law Rev. 126, 1904–1933.\n2. Solove DJ. 2007 ‘I’ve got nothing to hide’ and other misunderstandings of privacy. San Diego\nLaw Rev. 44, 745–772.\n16E.g., Article 29 Data Protection Working Party, Opinion 05/2014 on Anonymisation Techniques (2014).\n3. Gatewood G. 2001 Census Confidentiality and Privacy: 1790–2002. (http://www.census.gov/\nhistory/pdf/ConfidentialityMonograph.pdf)\n4. Abowd JM. 2017 Why the Census Bureau Adopted Differential Privacy for the 2020 Census\nof Population. Presentation at Harvard University, December 11.\n5. Swanson JA. 1992 The public and private in Aristotle’s political philosophy. Ithaca, NY: Cornell\nUniversity Press.\n6. Solove DJ. 2006 A taxonomy of privacy. Univ. PA. Law Rev. 154, 477–560. (doi:10.2307/400\n41279)\n7. Warren S, Brandeis L. 1890 The right to privacy. Harv. Law Rev. 4, 193–220. (doi:10.2307/132\n1160)\n8. Prosser WL. 1960 Privacy. Calif. Law Rev. 48, 383–423. (doi:10.2307/3478805)\n9. Solove DJ, Citron DK. 2018 Risk and anxiety: a theory of data breach harms. Tex. Law Rev. 96,\n737–786.\n10. US Dept. of Health, Education, & Welfare, Records, Computers, and the Rights of\nCitizens. 1973 Report of the Secretary’s Advisory Committee on Automated Personal Data\nSystems.\n11. Organisation for Economic Co-operation and Development. 2013 The OECD Privacy\nFramework.\n12. Gellman R. 2017 Fair information practices: a basic history.Working Paper, Version 2.18.\n13. Schwartz PM, Solove DJ. 2011 The PII problem: privacy and a new concept of personally\nidentifiable information. NYU Law Rev. 86, 1814–1894.\n14. Nissenbaum H. 2009 Privacy in context: technology, policy, and the integrity of social life. Redwood\nCity, CA: Stanford University Press.\n15. Obama Administration. 2015 Administration Discussion Draft: Consumer Privacy Bill of\nRights Act of 2015.\n16. Barth A, Datta A, Mitchell J, NissenbaumH. 2006 Privacy and contextual integrity: framework\nand applications. In Proc. of the 2006 IEEE Symp. on Security and Privacy, Berkeley, CA, 21–24\nMay, pp. 184–198. Washington, DC: IEEE Computer Society.\n17. Adam NR, Worthmann JC. 1989 Security-control methods for statistical databases: a\ncomparative study. ACM Comput. Surv. (CSUR) 21, 515–556. (doi:10.1145/76894.76895)\n18. Federal Committee on Statistical Methodology. 2005 Report on statistical disclosure limitation\nmethodology. Statistical PolicyWorking Paper 22.\n19. Sweeney L. 2000 Uniqueness of Simple Demographics in the U.S. Population. Laboratory for\nInt’l Data Privacy,Working Paper LIDAP-WP4.\n20. Narayanan A, Shmatikov V. 2008 Robust de-anonymization of large sparse datasets. In Proc.\nof the 2008 IEEE Symposium on Research in Security and Privacy, Oakland, CA, 18–21 May, p. 111.\nWashington, DC: IEEE Computer Society.\n21. de Montjoye YA, Hidalgo CA, Verleysen M, Blondel VD. 2013 Unique in the crowd: the\nprivacy bounds of human mobility. Nat. Sci. Rep. 3, 1376. (doi:10.1038/srep01376)\n22. Ohm P. 2010 Broken promises of privacy: responding to the surprising failure of\nanonymization. UCLA Law Rev. 57, 1701.\n23. Narayanan A, Shmatikov V. 2010 Myths and fallacies of ‘Personally Identifiable Information’.\nCommun. ACM 53, 24–26. (doi:10.1145/1743546.1743558)\n24. Sweeney L. 2002 k-anonymity. A model for protecting privacy. Int. J. Uncertain. Fuzziness\nKnowl.-Based Syst. 10, 557–570. (doi:10.1142/S0218488502001648)\n25. Machanavajjhala A, Kifer D, Gehrke J, Venkitasubramaniam M. 2007 l-diversity: privacy\nbeyond k-anonymity. ACM Trans. Knowl. Discov. Data. 1, 3.\n26. Li N, Li T, Venkatasubramanian S. 2007 t-closeness: privacy beyond k-anonymity and ldiversity.\nIn Proc. of the 23rd Int. Conf. on Data Engineering, ICDE, Istanbul, Turkey, 15–20 April,\npp. 106–115. Washington, DC: IEEE Computer Society.\n27. Goldwasser S, Micali S. 1984 Probabilistic encryption. J. Comput. Syst. Sci. 28, 270–299.\n(doi:10.1016/0022-0000(84)90070-9)\n28. Dalenius T. 1977 Towards a methodology for statistical disclosure control. Statistik Tidskrift 15,\n429–444.\n29. Dwork C, Naor M. 2010 On the difficulties of disclosure prevention in statistical databases or\nthe case for differential privacy. J. Priv. Confidentiality 2, 93–107. (doi:10.29012/jpc.v2i1.585)\n30. Lindell Y, Pinkas B. 2009 Secure multiparty computation for privacy-preserving data mining.\nJ. Priv. Confidentiality 1, 59–98. (doi:10.29012/jpc.v1i1.566)\n31. Gentry C. 2010 Computing arbitrary functions of encrypted data. Commun. ACM 53, 97–105.\n(doi:10.1145/1666420.1666444)\n32. Hayden E. 2015 Extreme cryptography paves way to personalized medicine. Nat. News 519,\n400–401. (doi:10.1038/519400a)\n33. Diffie W, Hellman M. 1976 New directions in cryptography. IEEE Trans. Inf. Theory 22, 644–\n654. (doi:10.1109/tit.1976.1055638)\n34. Dinur I, Nissim K. 2003 Revealing information while preserving privacy. In Proc. of the 22nd\nSymposium on Principles of database systems (PODS), San Diego, CA, 9–12 June, pp. 202–210. New\nYork, NY: ACM.\n35. Dwork C,McSherry F, Nissim K, Smith A. 2017 Calibrating noise to sensitivity in private data\nanalysis. J. Priv. Confidentiality 7, 17–51. (doi:10.29012/jpc.v7i3.405)\n36. Nissim K, Steinke T, Wood A, Altman M, Bembenek A, Bun M, Gaboardi M, O’Brien DR,\nVadhan S. To appear. Differential privacy: a primer for a non-technical audience. Vanderbilt J.\nEntertainment Technol. Law.\n37. Tang J, Korolova A, Bai X, Wang X, Wang X. 2017 Privacy loss in Apple’s implementation of\ndifferential privacy on MacOS 10.12. Working paper.\n38. Haney S, Machanavajjhala A, Abowd JM, Graham M, Kutzbach M, Vilhuber L. 2017 Utility\ncost of formal privacy for releasing national employer-employee statistics. In SIGMOD\nConference, Chicago, IL, 14–19 May, pp. 1339–1354. New York, NY: ACM.\n39. Nissim K, Bembenek A, Wood A, Bun M, Gaboardi M, Gasser U, O’Brien DR, Steinke T,\nVadhan S. 2018 Bridging the gap between computer science and legal approaches to privacy.\nHarv. J. Law & Technol., vol. 31, Number 2 Spring 2018, pp. 689–780.'
>>> for line in f:
...     print(line.strip())
... 
>>> import nltk
>>> f.concordance("privacy")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: '_io.TextIOWrapper' object has no attribute 'concordance'
>>> f = open("privacy.txt")
>>> f.concordance("privacy")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: '_io.TextIOWrapper' object has no attribute 'concordance'
>>> f
<_io.TextIOWrapper name='privacy.txt' mode='r' encoding='UTF-8'>
>>> type(f)
<class '_io.TextIOWrapper'>
>>> f = open("privacy.txt").read()
>>> type(f)
<class 'str'>
>>> import word_tokenize
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ModuleNotFoundError: No module named 'word_tokenize'
>>> from nltk import word_tokenize
>>> tokens = word_tokenize(f)
>>> type(tokens)
<class 'list'>
>>> tokens.findall(r"<a> (<.*>) <man>")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'list' object has no attribute 'findall'
>>> f = open("privacy.txt")
>>> f
<_io.TextIOWrapper name='privacy.txt' mode='r' encoding='UTF-8'>
>>> text1
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'text1' is not defined
>>> from urllib import request
>>> url = "http://www.gutenberg.org/cache/epub/37368/pg37368.txt"
>>> response = request.urlopen(url)
>>> raw = response.read().decode('utf8')
>>> type(raw)
<class 'str'>
>>> len(raw)
104151
>>> tokens = word_tokenize(raw)
>>> type(tokens)
<class 'list'>
>>> len(tokens)
19983
>>> tokens[:10]
['\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'The', 'Right', 'to', 'Privacy', ',']
>>> text = nltk.Text(tokens)
>>> type(text)
<class 'nltk.text.Text'>
>>> textt.collocations()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'textt' is not defined
>>> text.collocations()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/Users/christian/.local/lib/python3.7/site-packages/nltk/text.py", line 444, in collocations
    w1 + " " + w2 for w1, w2 in self.collocation_list(num, window_size)
  File "/Users/christian/.local/lib/python3.7/site-packages/nltk/text.py", line 444, in <listcomp>
    w1 + " " + w2 for w1, w2 in self.collocation_list(num, window_size)
ValueError: too many values to unpack (expected 2)
>>> raw.find("privacy")
5386
>>> text.concordance('privacy')
Displaying 25 of 42 matches:
ect Gutenberg EBook of The Right to Privacy , by Samuel D. Warren and Louis D. 
.gutenberg.net Title : The Right to Privacy Author : Samuel D. Warren Louis D. 
ROJECT GUTENBERG EBOOK THE RIGHT TO PRIVACY *** Produced by Jana Srna , Matthew
R 15 , 1890 . NO . 5 . THE RIGHT TO PRIVACY . `` It could be done only on princ
1 ] and the evil of the invasion of privacy by the newspapers , long keenly fel
 recognize and protect the right to privacy in this and in other respects must 
to publicity , so that solitude and privacy have become more essential to the i
n have , through invasions upon his privacy , subjected him to mental pain and 
 properly be invoked to protect the privacy of the individual ; and , if it doe
 nature of the instruments by which privacy is invaded , the injury inflicted b
 applicable to cases of invasion of privacy , to invoke the analogy , which is 
 applications of a general right to privacy , which properly understood afford 
ts in the case before him , that `` privacy is the right invaded . '' But if pr
cy is the right invaded . '' But if privacy is once recognized as a right entit
which may be invoked to protect the privacy of the individual from invasion eit
cisions indicate a general right to privacy for thoughts , emotions , and sensa
can be rested , except the right to privacy , as a part of the more general rig
nts of the letter , or his right to privacy . [ 37 ] A similar groping for the 
r of the emotions , is the right to privacy , and the law has no new principle 
herwise . [ 40 ] If the invasion of privacy constitutes a legal _injuria_ , the
depraved imagination . The right to privacy , limited as such right must necess
re the limitations of this right to privacy , and what remedies may be granted 
rtistic property . 1 . The right to privacy does not prohibit any publication o
 unwarranted invasion of individual privacy which is reprehended , and to be , 
al object in view is to protect the privacy of private life , and to whatever d
>>> 
